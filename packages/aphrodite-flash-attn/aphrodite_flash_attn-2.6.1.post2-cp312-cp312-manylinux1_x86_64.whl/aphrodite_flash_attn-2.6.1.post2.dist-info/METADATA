Metadata-Version: 2.1
Name: aphrodite-flash-attn
Version: 2.6.1.post2
Summary: Forward-only flash-attn with CUDA 12.4
Home-page: https://github.com/AlpinDale/flash-attention.git
Author: AlpinDale & vLLM Team & Tri Dao
Classifier: Programming Language :: Python :: 3
Classifier: License :: OSI Approved :: BSD License
Classifier: Operating System :: Unix
Requires-Python: >=3.8
License-File: LICENSE
License-File: AUTHORS
Requires-Dist: torch==2.4.0

Forward-only flash-attn package built for PyTorch 2.4.0 and CUDA 12.4
