{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gmft.pdf_bindings import PyPDFium2Document\n",
    "from gmft import CroppedTable, TableDetector\n",
    "\n",
    "detector = TableDetector()\n",
    "\n",
    "def ingest_pdf(pdf_path) -> list[CroppedTable]:\n",
    "    doc = PyPDFium2Document(pdf_path)\n",
    "\n",
    "    tables = []\n",
    "    for page in doc:\n",
    "        tables += detector.extract(page)\n",
    "    return tables, doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tables, doc = ingest_pdf(\"example_report.pdf\")\n",
    "tables, doc = ingest_pdf(\"/home/snexus/Downloads/2407.03291v1.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gmft.table_detection.CroppedTable"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tables[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gmft.pdf_bindings.bindings_pdfium.PyPDFium2Document"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gmft.pdf_bindings.bindings_pdfium.PyPDFium2Page"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page = doc.get_page(24)\n",
    "type(page)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page.page_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_text = page.page.get_textpage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VCHAR:Variance-Driven Complex Human Activity Recognition framework with Generative Representation • 0 :25\n",
      "[17] Harish Haresamudram, David V Anderson, and Thomas Plötz. 2019. On the role of features in human activity recognition. In Proceedings\n",
      "of the 2019 ACM International Symposium on Wearable Computers. 78–88.\n",
      "[18] Taeho Hur, Jaehun Bang, Thien Huynh-The, Jongwon Lee, Jee-In Kim, and Sungyoung Lee. 2018. Iss2Image: A novel signal-encoding\n",
      "technique for CNN-based human activity recognition. Sensors 18, 11 (2018), 3910.\n",
      "[19] Sozo Inoue, Paula Lago, Tahera Hossain, Tittaya Mairittha, and Nattaya Mairittha. 2019. Integrating activity recognition and nursing\n",
      "care records: The system, deployment, and a verification study. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous\n",
      "Technologies 3, 3 (2019), 1–24.\n",
      "[20] Chihiro Ito, Xin Cao, Masaki Shuzo, and Eisaku Maeda. 2018. Application of CNN for human activity recognition with FFT spectrogram\n",
      "of acceleration and gyro sensors. In Proceedings of the 2018 ACM international joint conference and 2018 international symposium on\n",
      "pervasive and ubiquitous computing and wearable computers. 1503–1510.\n",
      "[21] Jeya Vikranth Jeyakumar, Ankur Sarker, Luis Antonio Garcia, and Mani Srivastava. 2023. X-char: A concept-based explainable complex\n",
      "human activity recognition model. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies 7, 1 (2023), 1–28.\n",
      "[22] Wenchao Jiang and Zhaozheng Yin. 2015. Human activity recognition using wearable sensors by deep convolutional neural networks.\n",
      "In Proceedings of the 23rd ACM international conference on Multimedia. 1307–1310.\n",
      "[23] Pang Wei Koh, Thao Nguyen, Yew Siang Tang, Stephen Mussmann, Emma Pierson, Been Kim, and Percy Liang. 2020. Concept bottleneck\n",
      "models. In International conference on machine learning. PMLR, 5338–5348.\n",
      "[24] Hyeokhyen Kwon, Gregory D Abowd, and Thomas Plötz. 2019. Handling annotation uncertainty in human activity recognition. In\n",
      "Proceedings of the 2019 ACM International Symposium on Wearable Computers. 109–117.\n",
      "[25] Paula Lago, Shingo Takeda, Kohei Adachi, Sayeda Shamma Alia, Moe Matsuki, Brahim Benai, Sozo Inoue, and Francois Charpillet. 2020.\n",
      "Cooking activity dataset with macro and micro activities. https://doi.org/10.21227/hyzg-9m49\n",
      "[26] Wei Li, Can Gao, Guocheng Niu, Xinyan Xiao, Hao Liu, Jiachen Liu, Hua Wu, and Haifeng Wang. 2020. Unimo: Towards unified-modal\n",
      "understanding and generation via cross-modal contrastive learning. arXiv preprint arXiv:2012.15409 (2020).\n",
      "[27] Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, and Aniruddha Kembhavi. 2022. Unified-io: A unified model for vision,\n",
      "language, and multi-modal tasks. In The Eleventh International Conference on Learning Representations.\n",
      "[28] Scott M Lundberg and Su-In Lee. 2017. A unified approach to interpreting model predictions. Advances in neural information processing\n",
      "systems 30 (2017).\n",
      "[29] Sakorn Mekruksavanich and Anuchit Jitpattanakul. 2021. Deep convolutional neural network with rnns for complex activity recognition\n",
      "using wrist-worn wearable sensor data. Electronics 10, 14 (2021), 1685.\n",
      "[30] Sinno Jialin Pan and Qiang Yang. 2009. A survey on transfer learning. IEEE Transactions on knowledge and data engineering 22, 10 (2009),\n",
      "1345–1359.\n",
      "[31] Liangying Peng, Ling Chen, Menghan Wu, and Gencai Chen. 2018. Complex activity recognition using acceleration, vital sign, and\n",
      "location data. IEEE Transactions on Mobile Computing 18, 7 (2018), 1488–1498.\n",
      "[32] Liangying Peng, Ling Chen, Zhenan Ye, and Yi Zhang. 2018. Aroma: A deep multi-task learning based simple and complex human\n",
      "activity recognition method using wearable sensors. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies\n",
      "2, 2 (2018), 1–16.\n",
      "[33] Parisa Rashidi and Alex Mihailidis. 2012. A survey on ambient-assisted living tools for older adults. IEEE journal of biomedical and\n",
      "health informatics 17, 3 (2012), 579–590.\n",
      "[34] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. \" Why should i trust you?\" Explaining the predictions of any classifier. In\n",
      "Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining. 1135–1144.\n",
      "[35] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. 2022. High-resolution image synthesis with\n",
      "latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 10684–10695.\n",
      "[36] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. 2022. High-Resolution Image Synthesis With\n",
      "Latent Diffusion Models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 10684–10695.\n",
      "[37] Majd Saleh, Manuel Abbas, and Regine Bouquin Le Jeannes. 2020. FallAllD: An open dataset of human falls and activities of daily living\n",
      "for classical and deep learning applications. IEEE Sensors Journal 21, 2 (2020), 1849–1858.\n",
      "[38] Muhammad Shoaib, Stephan Bosch, Ozlem Durmaz Incel, Hans Scholten, and Paul JM Havinga. 2016. Complex human activity\n",
      "recognition using smartphone and wrist-worn motion sensors. Sensors 16, 4 (2016), 426.\n",
      "[39] Muhammad Shoaib, Hans Scholten, Paul JM Havinga, and Ozlem Durmaz Incel. 2016. A hierarchical lazy smoking detection algorithm\n",
      "using smartwatch sensors. In 2016 IEEE 18th International Conference on e-Health Networking, Applications and Services (Healthcom).\n",
      "IEEE, 1–6.\n",
      "[40] Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. 2013. Deep inside convolutional networks: Visualising image classification\n",
      "models and saliency maps. arXiv preprint arXiv:1312.6034 (2013).\n",
      "[41] Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, and Douwe Kiela. 2022.\n",
      "Flava: A foundational language and vision alignment model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\n",
      "Recognition. 15638–15650.\n",
      "Proc. ACM Interact. Mob. Wearable Ubiquitous Technol., Vol. 0, No. 0, Article 0 . Publication date: 2024.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/snexus/projects/llm-search/.venv/lib64/python3.11/site-packages/pypdfium2/_helpers/textpage.py:80: UserWarning: get_text_range() call with default params will be implicitly redirected to get_text_bounded()\n",
      "  warnings.warn(\"get_text_range() call with default params will be implicitly redirected to get_text_bounded()\")\n"
     ]
    }
   ],
   "source": [
    "print(page_text.get_text_range())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p1 = doc.page\n",
    "# help(p1.get_positions_and_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gmft.pdf_bindings.bindings_pdfium.PyPDFium2Document at 0x7fa7e3f93b10>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for table in tables:\n",
    "#     table.visualize(figsize=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gmft import AutoTableFormatter, TATRTableFormatter\n",
    "\n",
    "formatter = TATRTableFormatter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method extract in module gmft.table_function:\n",
      "\n",
      "extract(table: gmft.table_detection.CroppedTable, dpi=144, padding='auto') -> gmft.table_function.FormattedTable method of gmft.table_function.TATRTableFormatter instance\n",
      "    Extract the data from the table.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(formatter.extract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_index = 2\n",
    "table = tables[table_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "CroppedTable.from_image_only() missing 1 required positional argument: 'img'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_image_only\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: CroppedTable.from_image_only() missing 1 required positional argument: 'img'"
     ]
    }
   ],
   "source": [
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The identified boxes have significant overlap: 216.11% of area is overlapping (Max is 20.00%)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m ft \u001b[38;5;241m=\u001b[39m formatter\u001b[38;5;241m.\u001b[39mextract(tables[table_index], dpi\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mft\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisualize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/llm-search/.venv/lib64/python3.11/site-packages/gmft/table_function.py:317\u001b[0m, in \u001b[0;36mTATRFormattedTable.visualize\u001b[0;34m(self, filter, dpi, effective, **kwargs)\u001b[0m\n\u001b[1;32m    315\u001b[0m     dpi \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m72\u001b[39m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    318\u001b[0m scale_by \u001b[38;5;241m=\u001b[39m (dpi \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m72\u001b[39m)\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m effective:\n",
      "File \u001b[0;32m~/projects/llm-search/.venv/lib64/python3.11/site-packages/gmft/table_function.py:300\u001b[0m, in \u001b[0;36mTATRFormattedTable.df\u001b[0;34m(self, config_overrides)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    298\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\n\u001b[0;32m--> 300\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df \u001b[38;5;241m=\u001b[39m \u001b[43mextract_to_df\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df\n",
      "File \u001b[0;32m~/projects/llm-search/.venv/lib64/python3.11/site-packages/gmft/table_function.py:855\u001b[0m, in \u001b[0;36mextract_to_df\u001b[0;34m(table, config)\u001b[0m\n\u001b[1;32m    852\u001b[0m total_area \u001b[38;5;241m=\u001b[39m (total_row_area \u001b[38;5;241m+\u001b[39m total_column_area) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m total_area \u001b[38;5;241m>\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m table\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mtotal_overlap_reject_threshold) \u001b[38;5;241m*\u001b[39m table_area:\n\u001b[0;32m--> 855\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe identified boxes have significant overlap: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_area\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39mtable_area\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2%\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of area is overlapping (Max is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mtotal_overlap_reject_threshold\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2%\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    857\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m total_area \u001b[38;5;241m>\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m table\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mtotal_overlap_warn_threshold) \u001b[38;5;241m*\u001b[39m table_area:\n\u001b[1;32m    858\u001b[0m     outliers[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhigh overlap\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m (total_area \u001b[38;5;241m/\u001b[39m table_area \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: The identified boxes have significant overlap: 216.11% of area is overlapping (Max is 20.00%)"
     ]
    }
   ],
   "source": [
    "\n",
    "ft = formatter.extract(tables[table_index], dpi=200)\n",
    "ft.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft = formatter.extract(tables[table_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ft.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The identified boxes have significant overlap: 215.94% of area is overlapping (Max is 20.00%)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[68], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mft\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39minfo()\n",
      "File \u001b[0;32m~/projects/llm-search/.venv/lib64/python3.11/site-packages/gmft/table_function.py:300\u001b[0m, in \u001b[0;36mTATRFormattedTable.df\u001b[0;34m(self, config_overrides)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    298\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\n\u001b[0;32m--> 300\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df \u001b[38;5;241m=\u001b[39m \u001b[43mextract_to_df\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df\n",
      "File \u001b[0;32m~/projects/llm-search/.venv/lib64/python3.11/site-packages/gmft/table_function.py:855\u001b[0m, in \u001b[0;36mextract_to_df\u001b[0;34m(table, config)\u001b[0m\n\u001b[1;32m    852\u001b[0m total_area \u001b[38;5;241m=\u001b[39m (total_row_area \u001b[38;5;241m+\u001b[39m total_column_area) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m total_area \u001b[38;5;241m>\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m table\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mtotal_overlap_reject_threshold) \u001b[38;5;241m*\u001b[39m table_area:\n\u001b[0;32m--> 855\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe identified boxes have significant overlap: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_area\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39mtable_area\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2%\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of area is overlapping (Max is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mtotal_overlap_reject_threshold\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2%\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    857\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m total_area \u001b[38;5;241m>\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m table\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mtotal_overlap_warn_threshold) \u001b[38;5;241m*\u001b[39m table_area:\n\u001b[1;32m    858\u001b[0m     outliers[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhigh overlap\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m (total_area \u001b[38;5;241m/\u001b[39m table_area \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: The identified boxes have significant overlap: 215.94% of area is overlapping (Max is 20.00%)"
     ]
    }
   ],
   "source": [
    "ft.df().info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ft.df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "df.columns =  [re.sub(r\"\\W+\",\"\", col)+str(i) for i, col in enumerate(df.columns) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>m1</th>\n",
       "      <th>m2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Shop fits for new and existing stores</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Production equipment and tooling</td>\n",
       "      <td>9.3</td>\n",
       "      <td>10.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Computer equipment and software</td>\n",
       "      <td>2.1</td>\n",
       "      <td>2.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Site</td>\n",
       "      <td>1.9</td>\n",
       "      <td>3.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Total capital additions</td>\n",
       "      <td>14.6</td>\n",
       "      <td>17.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       0    m1    m2\n",
       "0  Shop fits for new and existing stores   1.3   1.3\n",
       "1       Production equipment and tooling   9.3  10.1\n",
       "2        Computer equipment and software   2.1   2.9\n",
       "3                                   Site   1.9   3.4\n",
       "4                Total capital additions  14.6  17.7"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5 entries, 0 to 4\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   0       5 non-null      object\n",
      " 1   m1      5 non-null      object\n",
      " 2   m2      5 non-null      object\n",
      "dtypes: object(3)\n",
      "memory usage: 252.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "ft.df().info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<table border=\"1\" class=\"dataframe\">\\n  <thead>\\n    <tr style=\"text-align: right;\">\\n      <th></th>\\n      <th>0</th>\\n      <th>m1</th>\\n      <th>m2</th>\\n    </tr>\\n  </thead>\\n  <tbody>\\n    <tr>\\n      <th>0</th>\\n      <td>Shop fits for new and existing stores</td>\\n      <td>1.3</td>\\n      <td>1.3</td>\\n    </tr>\\n    <tr>\\n      <th>1</th>\\n      <td>Production equipment and tooling</td>\\n      <td>9.3</td>\\n      <td>10.1</td>\\n    </tr>\\n    <tr>\\n      <th>2</th>\\n      <td>Computer equipment and software</td>\\n      <td>2.1</td>\\n      <td>2.9</td>\\n    </tr>\\n    <tr>\\n      <th>3</th>\\n      <td>Site</td>\\n      <td>1.9</td>\\n      <td>3.4</td>\\n    </tr>\\n    <tr>\\n      <th>4</th>\\n      <td>Total capital additions</td>\\n      <td>14.6</td>\\n      <td>17.7</td>\\n    </tr>\\n  </tbody>\\n</table>'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.to_records()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(row):\n",
    "    xml = ['<item>']\n",
    "    for field in row.index:\n",
    "        xml.append('  <field name=\"{0}\">{1}</field>'.format(field, row[field]))\n",
    "    xml.append('</item>')\n",
    "    return '\\n'.join(xml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    <item>\\n  <field name=\"0\">Shop fits for new an...\n",
       "1    <item>\\n  <field name=\"0\">Production equipment...\n",
       "2    <item>\\n  <field name=\"0\">Computer equipment a...\n",
       "3    <item>\\n  <field name=\"0\">Site</field>\\n  <fie...\n",
       "4    <item>\\n  <field name=\"0\">Total capital additi...\n",
       "dtype: object"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.apply(func, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<item>\n",
      "  <field name=\"0\">Shop fits for new and existing stores</field>\n",
      "  <field name=\"m1\">1.3</field>\n",
      "  <field name=\"m2\">1.3</field>\n",
      "</item>\n",
      "<item>\n",
      "  <field name=\"0\">Production equipment and tooling</field>\n",
      "  <field name=\"m1\">9.3</field>\n",
      "  <field name=\"m2\">10.1</field>\n",
      "</item>\n",
      "<item>\n",
      "  <field name=\"0\">Computer equipment and software</field>\n",
      "  <field name=\"m1\">2.1</field>\n",
      "  <field name=\"m2\">2.9</field>\n",
      "</item>\n",
      "<item>\n",
      "  <field name=\"0\">Site</field>\n",
      "  <field name=\"m1\">1.9</field>\n",
      "  <field name=\"m2\">3.4</field>\n",
      "</item>\n",
      "<item>\n",
      "  <field name=\"0\">Total capital additions</field>\n",
      "  <field name=\"m1\">14.6</field>\n",
      "  <field name=\"m2\">17.7</field>\n",
      "</item>\n"
     ]
    }
   ],
   "source": [
    "print('\\n'.join(df.apply(func, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6 entries, 0 to 5\n",
      "Data columns (total 7 columns):\n",
      " #   Column                                        Non-Null Count  Dtype \n",
      "---  ------                                        --------------  ----- \n",
      " 0                                                 5 non-null      object\n",
      " 1   Number of stores at 29 May 2022               6 non-null      object\n",
      " 2   Opened                                        6 non-null      object\n",
      " 3   Closed                                        6 non-null      object\n",
      " 4   Number of stores at 28 May 2023               6 non-null      object\n",
      " 5   Number of single staff stores at 28 May 2023  6 non-null      object\n",
      " 6   Number of single staff stores at 29 May 2022  6 non-null      object\n",
      "dtypes: object(7)\n",
      "memory usage: 468.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "ft.df().info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
