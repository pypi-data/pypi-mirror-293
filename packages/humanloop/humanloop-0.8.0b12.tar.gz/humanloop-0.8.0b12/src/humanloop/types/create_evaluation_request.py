# This file was auto-generated by Fern from our API Definition.

from ..core.unchecked_base_model import UncheckedBaseModel
from .evaluations_dataset_request import EvaluationsDatasetRequest
import pydantic
import typing
from .evaluatee_request import EvaluateeRequest
from .evaluations_request import EvaluationsRequest
from ..core.pydantic_utilities import IS_PYDANTIC_V2


class CreateEvaluationRequest(UncheckedBaseModel):
    """
    Request model for creating an Evaluation.

    Evaluation benchmark your Prompt/Tool Versions. With the Datapoints in a Dataset Version,
    Logs corresponding to the Datapoint and each Evaluated Version are evaluated by the specified Evaluator Versions.
    Aggregated statistics are then calculated and presented in the Evaluation.
    """

    dataset: EvaluationsDatasetRequest = pydantic.Field()
    """
    The Dataset Version to use in this Evaluation.
    """

    evaluatees: typing.Optional[typing.List[EvaluateeRequest]] = pydantic.Field(default=None)
    """
    Unique identifiers for the Prompt/Tool Versions to include in the Evaluation Report. Can be left unpopulated if you wish to add evaluatees to this Evaluation Report by specifying `evaluation_id` in Log calls.
    """

    evaluators: typing.List[EvaluationsRequest] = pydantic.Field()
    """
    The Evaluators used to evaluate.
    """

    if IS_PYDANTIC_V2:
        model_config: typing.ClassVar[pydantic.ConfigDict] = pydantic.ConfigDict(extra="allow", frozen=True)  # type: ignore # Pydantic v2
    else:

        class Config:
            frozen = True
            smart_union = True
            extra = pydantic.Extra.allow
