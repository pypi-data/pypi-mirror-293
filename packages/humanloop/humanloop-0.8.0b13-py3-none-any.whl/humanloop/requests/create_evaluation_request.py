# This file was auto-generated by Fern from our API Definition.

import typing_extensions
from .evaluations_dataset_request import EvaluationsDatasetRequestParams
import typing_extensions
import typing
from .evaluatee_request import EvaluateeRequestParams
from .evaluations_request import EvaluationsRequestParams


class CreateEvaluationRequestParams(typing_extensions.TypedDict):
    """
    Request model for creating an Evaluation.

    Evaluation benchmark your Prompt/Tool Versions. With the Datapoints in a Dataset Version,
    Logs corresponding to the Datapoint and each Evaluated Version are evaluated by the specified Evaluator Versions.
    Aggregated statistics are then calculated and presented in the Evaluation.
    """

    dataset: EvaluationsDatasetRequestParams
    """
    The Dataset Version to use in this Evaluation.
    """

    evaluatees: typing_extensions.NotRequired[typing.Sequence[EvaluateeRequestParams]]
    """
    Unique identifiers for the Prompt/Tool Versions to include in the Evaluation Report. Can be left unpopulated if you wish to add evaluatees to this Evaluation Report by specifying `evaluation_id` in Log calls.
    """

    evaluators: typing.Sequence[EvaluationsRequestParams]
    """
    The Evaluators used to evaluate.
    """
