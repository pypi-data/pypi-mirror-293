{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useckit High-Level Example\n",
    "\n",
    "This notebook provides an introduction to useckit. It focuses on the high-level API that useckit provides in the form of `Evaluators`. For this reason, we provide a dataset that we analyze further below.  We implement preprocessing functionality and run the final evaluation with `useckit`.\n",
    "\n",
    "To run this notebook, please first create a virtual environment (step 1), activate it (step 2) and install the requirements (step 3).\n",
    "1. Create virtual environment: `python -m venv venv`\n",
    "2. On Microsoft Windows and Powershell: `.\\venv\\Scripts\\activate.ps1`, on Unix-like systems with Bash: `source .\\venv\\bin\\activate`\n",
    "3. Install dependencies: `pip install -r requirements.txt`\n",
    "\n",
    "**Tested with**: `Python 3.10.7 (tags/v3.10.7:6cc6b13, Sep  5 2022, 14:08:36) [MSC v.1933 64 bit (AMD64)] on win32` and tensorflow `2.11.0`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Essential Imports and Setup\n",
    "\n",
    "We import several libraries from either python core and regular dependencies such as numpy, pandas, and matplotlib. We further provide a convenience-wrapper for `tqdm`, as some terminals (especially on Windows) do not provide sufficient support, thus we wrap `tqdm()` to always print ascii output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "\n",
    "\n",
    "def tqdm(*args):  # wrapper for tqdm to always print ascii output\n",
    "    \"\"\"Provide a progress bar for iterator operations.\"\"\"\n",
    "    from tqdm import tqdm as _tqdm\n",
    "    return _tqdm(args[0], ascii=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append current working directory to path\n",
    "# this is necessary so that jupyter interpreter can find the useckit lib to import\n",
    "# when opening this project where the folder `examples` is located beneath another root directory.\n",
    "sys.path.append(os.path.dirname(os.getcwd()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Extracting Dataset\n",
    "\n",
    "In the following, we are going to download an excerpt of a behavioral biometric dataset.\n",
    "\n",
    "### Origin of dataset\n",
    "\n",
    "The dataset originates from the following publication:\n",
    "\n",
    "```\n",
    "@article{doi:10.1080/10447318.2022.2120845,\n",
    "  author = {Jonathan Liebers and Sascha Brockel and Uwe Gruenefeld and Stefan Schneegass},\n",
    "  title = {Identifying Users by Their Hand Tracking Data in Augmented and Virtual Reality},\n",
    "  journal = {International Journal of Humanâ€“Computer Interaction},\n",
    "  volume = {0},\n",
    "  number = {0},\n",
    "  pages = {1-16},\n",
    "  year  = {2022},\n",
    "  publisher = {Taylor & Francis},\n",
    "  doi = {10.1080/10447318.2022.2120845},\n",
    "  URL = {https://doi.org/10.1080/10447318.2022.2120845},\n",
    "  eprint = {https://doi.org/10.1080/10447318.2022.2120845}\n",
    "}\n",
    "```\n",
    "\n",
    "We download the dataset from the internet. This script automatically downloads the file, checks the hashsum and extracts it.\n",
    "\n",
    "### Contents of dataset\n",
    "\n",
    "The dataset consists of 16 participants who performed a button-press in Augmented- and Reality. They repeated the button-press in total 12 times per session of the study. The study consisted of two sessions, where each session took place on a different day. We only focus on Virtual Reality here, where the data was elicited through a Meta Quest 2 device. The interaction of the participants took place through hand tracking, i.e., they interacted with the buttons with their tracked hands and fingers and without any controller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished extracting dataset to `dataset-decompressed`.\n"
     ]
    }
   ],
   "source": [
    "# Unzip\n",
    "\n",
    "dataset_dir = \"./dataset-decompressed/\"\n",
    "print(\"Testing for files and deciding whether to download and extract dataset ...\")\n",
    "download_and_unzip_dataset = len(glob(f'{dataset_dir}*tsv')) != 768\n",
    "\n",
    "if not os.path.exists(dataset_dir):\n",
    "    print('Creating dataset_dir', dataset_dir, 'since it does not exist.')\n",
    "    os.makedirs(dataset_dir)\n",
    "\n",
    "if download_and_unzip_dataset:\n",
    "    print(\"Downloading dataset ...\")\n",
    "    import urllib.request\n",
    "    urllib.request.urlretrieve(\"http://download-useckit-dataset.blindforreview.com\", \"dataset.zip\")\n",
    "\n",
    "    print(\"Checking hashsum ...\")\n",
    "    import hashlib\n",
    "    with open(\"dataset.zip\", \"rb\") as f:\n",
    "        bytes = f.read() # read entire file as bytes\n",
    "        readable_hash = hashlib.sha256(bytes).hexdigest()\n",
    "        print(\"sha256sum of dataset.zip is\", readable_hash)\n",
    "        assert readable_hash == \"ffb857f0958ee6de80b740e0e7fc25b3ae334ed9255aa54508fc279e604df06e\", \\\n",
    "            'hashsum did not match expectation. Please try the download again.'\n",
    "    print(\"sha256sum check of dataset.zip is ok.\")\n",
    "\n",
    "    print(\"Unzipping dataset ...\")\n",
    "    import zipfile\n",
    "    with zipfile.ZipFile(\"dataset.zip\", \"r\") as dataset_zipped:\n",
    "        dataset_zipped.extractall(\"dataset-decompressed\")\n",
    "\n",
    "    print(\"Finished extracting dataset to `dataset-decompressed`.\")\n",
    "else:\n",
    "    print(\"Dataset already extracted to ./dataset-decompressed/*tsv. Found 768 files. Skipping extraction of dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load dataset into memory\n",
    "\n",
    "In the following cell, we are going to load the extracted dataset into the jupyter interpreter and thus the memory the computer. Here, we load the files' contents together with some metadata for easier selections later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tsv_sample(path: str):\n",
    "    df = pd.read_csv(path, sep='\\t').drop(columns=\"Unnamed: 0\")\n",
    "\n",
    "    # extract information from each filename\n",
    "    basename_split = os.path.basename(path).split('_')\n",
    "    key_scene = basename_split[0].replace('SCENE-', '')\n",
    "    key_xr = basename_split[1].replace('XR-', '')\n",
    "    key_participant_id = int(basename_split[2].replace('PID-', ''))\n",
    "    key_repetition = int(basename_split[3].replace('REP-', ''))  #\n",
    "    key_session = int(basename_split[4].replace('SESSION-', '').replace('.tsv', ''))  # either '1' or '2' (i.e., first or second study session)\n",
    "\n",
    "    return {'scene': key_scene, 'xr': key_xr, 'pid': key_participant_id, 'rep': key_repetition, 'sess': key_session, 'df': df}\n",
    "\n",
    "DATASET_LIST = [load_tsv_sample(s) for s in tqdm(sorted(glob('dataset-decompressed/SCENE-ButtonScene*XR-vr*tsv')))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Inspect data and check assumptions\n",
    "\n",
    "In the following, we perform a short inspection of the dataset. Particularly, we create boxplots of the loaded data length and calculate descriptive statistics to better understand the interactions in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the following assumptions:\n",
    "# 1. we only load vr-data from the dataset (no ar data)\n",
    "assert all([x['xr'] == 'vr' for x in DATASET_LIST])\n",
    "\n",
    "# 2. we only load button-scene data\n",
    "assert all([x['scene'] == 'ButtonScene1H' for x in DATASET_LIST])\n",
    "\n",
    "# 3. we load data for 16 participants with 2 sessions with 12 repetitions each\n",
    "assert len(DATASET_LIST) == 16*2*12\n",
    "for pid in range(0, 15+1):\n",
    "    for session in [1, 2]:\n",
    "        for repetition in range(1, 12+1):\n",
    "            assert len([d for d in DATASET_LIST if d['pid'] == pid and d['sess'] == session and d['rep'] == repetition]) == 1, \\\n",
    "                   f'Did not find pid {pid}, session {session}, repetition {repetition} in DATASET_LIST.'\n",
    "\n",
    "# 4. we plot the length of each sample in a boxplot\n",
    "from useckit.util.utils import analyze_samplelength\n",
    "\n",
    "analyze_samplelength(DATASET_LIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Preprocessing\n",
    "\n",
    "In the following, we perform a pre-processing of the data.\n",
    "\n",
    "The pre-processing consists of the following steps:\n",
    "a. Coordinate transformations\n",
    "b. Interaction extractions\n",
    "c. Right hand data extraction\n",
    "d. Handling of tracking loss (NaN replacements.\n",
    "e. Min-Max Normalization, and\n",
    "f. Padding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5a: Coordinate Transformation\n",
    "\n",
    "We perform a coordinate transformation to actually focus on the human behavior in the data. For this, we subtract every column of the tabular data from its very first value. This allows us to make the data invariant to its original position.\n",
    "\n",
    "For example, body height (denoted by the y-axis of the HMD) is a very strong biometric feature. However, it is more of a physiological biometric feature than a behavioral one. Humans can be easily distinguished by their body height. By subtracting all values in the column from the very first value, we can remove the initial value and rather focus on changes in body height, in case of the y-axis of the HMD. This works for all other columns similarly, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in tqdm(DATASET_LIST):\n",
    "    for c in d['df'].columns:\n",
    "        if 'position_' in c:\n",
    "            d['df'][c] -= d['df'][c].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5b: Extract Interactions\n",
    "\n",
    "Each interaction has a different length, as seen by each file having a different number of lines. To process these files in deep learning, we need to unify the shape and thus the lengths of the various data files. First, we extract the actual human interaction from the file by checking for certain `Unity.Events` such as `OnButtonTouchBegin()` and `OnButtonTouchEnd()`. We consider all values between these two markers and additionally +/- 1 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interaction_extractor_button_scene(df: pd.DataFrame) -> tuple:\n",
    "    \"\"\"Find when the button in the button scene was touched in df's `Unity.Event` column and when it was let go. Return indexes of this interaction.\"\"\"\n",
    "    # define start searchstring and determine its index\n",
    "\n",
    "    # replace NaNs in Unity.Event column with emptystr\n",
    "    df['Unity.Event'] = df['Unity.Event'].fillna(value='')\n",
    "\n",
    "    start_searchstr = 'OnButtonTouchBegin()'\n",
    "    startidx = df.loc[df['Unity.Event'].str.contains(start_searchstr, regex=False)].index[0]\n",
    "\n",
    "    # define end searchstring and determine its index\n",
    "    end_searchstr = 'OnButtonTouchEnd()'\n",
    "    endidx = df.loc[df['Unity.Event'].str.contains(end_searchstr, regex=False)].index[0]\n",
    "\n",
    "    return startidx, endidx\n",
    "\n",
    "# create a positive and negative offset\n",
    "start_offset = -72\n",
    "stop_offset = abs(start_offset)\n",
    "\n",
    "for d in DATASET_LIST:\n",
    "    start_idx, stop_idx = interaction_extractor_button_scene(d['df'])\n",
    "    start = start_idx + start_offset\n",
    "    stop = stop_idx + stop_offset\n",
    "    if start < 0:\n",
    "        start = 0\n",
    "    assert 0 <= start < stop\n",
    "\n",
    "    d['df'] = d['df'].iloc[start:stop]\n",
    "\n",
    "    if len(d['df']) < 10:\n",
    "        print(f'Warning: sample {d} has a length of {len(d[\"df\"])} with start_idx {start_idx}, stop_idx {stop_idx}')\n",
    "\n",
    "    del start_idx, stop_idx, start, stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check the extracted interactions' lengths by creating a boxplot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptive_stats = analyze_samplelength(DATASET_LIST)\n",
    "descriptive_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5c: Extract Right Hand Interaction Only\n",
    "\n",
    "As all participants in the original study were right-handed, we intend to focus only on the righ thand's position (Euler coordinates) and orientation (Quaternion). Subsequently, we want to remove all other columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine columns that actually belong to the user's hands\n",
    "# we only select the right hand here as all participants were right-handed and thus interacted with the right hand with the button\n",
    "hand_columns = [c for c in DATASET_LIST[0]['df'].columns if 'R_' in c and ('.position' in c or '.rotation.quaternion' in c)]\n",
    "#print('\\n'.join(hand_columns))\n",
    "\n",
    "# apply these columns, i.e., remove all data that does not have a connection to user's hands\n",
    "for d in tqdm(DATASET_LIST):\n",
    "    d['df'] = d['df'][hand_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5d: Handle Tracking Loss\n",
    "\n",
    "In case the tracking of the Meta Quest 2 fails, no values are provided that are interpreted as not a number (NaN). We siply replace those values with 0f."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fillna\n",
    "for d in tqdm(DATASET_LIST):\n",
    "    d['df'] = d['df'].fillna(value=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5e: Apply MinMax normalization\n",
    "\n",
    "We apply a MinMax to fit all values from the elicited data into the interval $[0; 1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.preprocessing\n",
    "\n",
    "for d in tqdm(DATASET_LIST):\n",
    "    d['df'] = sklearn.preprocessing.MinMaxScaler().fit_transform(d['df'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5f: Apply Padding\n",
    "\n",
    "At last, we apply a pre-padding to unify the shape of the data. Until now, each dataframe had a varying length. We chose a `maxlen` of 180 which corresponds to the mean plus standard deviation of the boxplot above. Also, at the end of the preprocessing, we export the data into numpy arrays for faster processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = descriptive_stats[\"mean\"] + descriptive_stats[\"sd\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import pad_sequences  # https://www.tensorflow.org/api_docs/python/tf/keras/utils/pad_sequences\n",
    "DATA: np.array = pad_sequences(sequences=[d['df'] for d in DATASET_LIST],\n",
    "                               maxlen=maxlen,\n",
    "                               dtype='float16',\n",
    "                               padding='pre',\n",
    "                               truncating='pre',\n",
    "                               value=0.0)\n",
    "LABELS: np.array = np.array([d['pid'] for d in DATASET_LIST])\n",
    "SESSION: np.array = np.array([d['sess'] for d in DATASET_LIST])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Creating a useckit dataset\n",
    "\n",
    "Here, we create a Dataset object of useckit. It helps us to specify into which sets we want to assign our data. Particularly, we want to put the Session 1 into training. Nex,t we want to use it also for testing but as the enrollment dataset. For determining the final performance metrics, we put the second session into the matching dataset.\n",
    "\n",
    "Since we do not specify a specific part for validation, the enrollment data will be used also for validating the trained models. However, this is not an issue, since we perform a hold-out validation anyways, where the second session is reserved for testing by remaining unseen during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import useckit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = useckit.Dataset(trainset_data=DATA[SESSION == 1],\n",
    "                          trainset_labels=LABELS[SESSION == 1],\n",
    "                          testset_enrollment_data=DATA[SESSION == 1],\n",
    "                          testset_enrollment_labels=LABELS[SESSION == 1],\n",
    "                          testset_matching_data=DATA[SESSION == 2],\n",
    "                          testset_matching_labels=LABELS[SESSION == 2],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "DATA[SESSION == 1][0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we set some essential useckit parameters. First, we disable verbose output and we train for 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose=False\n",
    "epochs=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Training and Evaluation with useckit\n",
    "\n",
    "In the following, we are going to fit the deep learning models in useckit's paradigms to our data. By calling the evaluation methods, we are going to generate our final metrics. Useckit contains support for TSC Classifications (cf., Step 7a), Distance Metric trainings (cf., Step 7b), and Anomaly Detection techniques (cf., Step 7c)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7a: Training the useckit TSC Evaluator\n",
    "\n",
    "The TSC Evaluator performs closed-set identification using time-series classification techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "from useckit.Evaluators import TSCEvaluator\n",
    "\n",
    "tsc = TSCEvaluator(DATASET, epochs=epochs, verbose=verbose)\n",
    "tsc.evaluate()\n",
    "print(\"Evaluating TSC finished. Check your disk for results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7b: Distance Learning\n",
    "\n",
    "Next, we train and evaluate the distance learning functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "from useckit.Evaluators import DistanceLearningEvaluator\n",
    "\n",
    "dle = DistanceLearningEvaluator(DATASET, epochs=epochs, verbose=verbose)\n",
    "dle.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7c: Anomaly Detection\n",
    "\n",
    "At last, we train and evaluate the autoencoder-based anomaly detectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "from useckit.Evaluators import AnomalyDetectionEvaluator\n",
    "\n",
    "ade = AnomalyDetectionEvaluator(DATASET, epochs=epochs, verbose=verbose)\n",
    "ade.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "print(\"Execution finished.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
