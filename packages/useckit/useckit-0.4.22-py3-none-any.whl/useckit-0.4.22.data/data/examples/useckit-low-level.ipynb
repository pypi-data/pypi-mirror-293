{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# How to UsecKit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates the usage of the useckit library. Every output of this notebook will be placed in a folder named \"_uscekit_out\". To customize this behaviour change the output_dir attribute of the paradigm, evaluation method or prediction model instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following snippet of code creates a dataset with samples of shape (100, 100, 4) and labels equal to \"User_{x}\". While the data samples need to be uniformly shaped numpy arrays of number types, the labels can be numpy arrays of any objects, for example strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.dirname(os.getcwd()))  # fix jupyter path\n",
    "\n",
    "from useckit import Dataset\n",
    "from useckit.util.utils import make_some_intelligent_noise\n",
    "\n",
    "x_train, y_train = make_some_intelligent_noise()\n",
    "x_val, y_val = make_some_intelligent_noise()\n",
    "x_test_enroll, y_test_enroll = make_some_intelligent_noise()\n",
    "x_test_match, y_test_match = make_some_intelligent_noise()\n",
    "\n",
    "dataset = Dataset(trainset_data=x_train,\n",
    "                  trainset_labels=y_train,\n",
    "                  validationset_data=x_val,\n",
    "                  validationset_labels=y_val,\n",
    "                  testset_enrollment_data=x_test_enroll,\n",
    "                  testset_enrollment_labels=y_test_enroll,\n",
    "                  testset_matching_data=x_test_match,\n",
    "                  testset_matching_labels=y_test_match,\n",
    "                  normalization_check=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Paradigm and evaluate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from useckit.paradigms.distance_learning.distance_paradigm import DistanceMetricParadigm\n",
    "\n",
    "paradigm_1 = DistanceMetricParadigm()\n",
    "paradigm_1.evaluate(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restore the trained model from the previous paradigm to evaluate it with different evaluation methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from useckit.paradigms.distance_learning.distance_paradigm import DistanceMetricParadigm\n",
    "from useckit.paradigms.distance_learning.evaluation_methods.verification import Verification as DistanceVerification\n",
    "from useckit.paradigms.distance_learning.evaluation_methods.identification_with_reject import IdentificationWithReject as DistanceIdentificationWithReject\n",
    "\n",
    "\n",
    "saved_weights_path = os.path.join(paradigm_1.prediction_model.output_dir, 'best_model.hdf5')\n",
    "paradigm_2 = DistanceMetricParadigm(evaluation_methods=[DistanceIdentificationWithReject(), DistanceVerification()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are not calling the evaluate method, hence we need to save the paradigm manually (if we so wish). This always needs to be done before fitting the paradigm's model or before restoring the paradigm's model's trainable values as we will be doing after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paradigm_2.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we restore the paradigm's model's trainable values from the previous fitting during the evaluation of the first paradigm. Afterwards we run the evaluation methods of the paradigm with the already existing dataset. Note that we need to give the dataset as parameter for the restoration of the model as the paradigm uses it to infer among others the model's input layer size. Using a dataset with differently shaped samples will fail this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paradigm_2.restore_prediction_model_trainables(saved_weights_path_like=saved_weights_path, dataset=dataset)\n",
    "paradigm_2.run_evaluation_methods(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate multiple paradigms with different prediction models to compare them amongst each other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principally, we can combine any evaluation_method with any prediction_model from the respective sub-packages of the paradigms. Due to the single implementation of the different biometric tasks in the evaluation package, which is used by the evaluation_methods of all paradigms, the results are comparable not only within one, but across the paradigms as well. The below code snippet shows a small example of how to create three comparable experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from useckit.paradigms.time_series_classification.prediction_models.keras_model_descriptions import dl4tsc_mlp\n",
    "from useckit.paradigms.time_series_classification.prediction_models.inception import dl4tsc_inception\n",
    "from useckit.paradigms.time_series_classification.prediction_models.classification_keras_prediction_model import ClassificationKerasPredictionModel as TSCKerasModel\n",
    "from useckit.paradigms.time_series_classification.evaluation_methods.identification import IdentificationOnly as TSCIdentification\n",
    "from useckit.paradigms.time_series_classification.tsc_paradigm import TSCParadigm\n",
    "from useckit.paradigms.anomaly_detection.prediction_models.auto_encoder_keras_prediction_model import AutoEncoderKerasPredictionModel as AnomalyKerasModel\n",
    "from useckit.paradigms.anomaly_detection.evaluation_methods.identification import IdentificationOnly as AnomalyIdentification\n",
    "from useckit.paradigms.anomaly_detection.anomaly_paradigm import AnomalyParadigm\n",
    "\n",
    "x_train, y_train = make_some_intelligent_noise(shape=(40, 10, 10))\n",
    "x_val, y_val = make_some_intelligent_noise(shape=(40, 10, 10))\n",
    "x_test_enroll, y_test_enroll = make_some_intelligent_noise(shape=(40, 10, 10))\n",
    "x_test_match, y_test_match = make_some_intelligent_noise(shape=(40, 10, 10))\n",
    "\n",
    "dataset = Dataset(trainset_data=x_train,\n",
    "                  trainset_labels=y_train,\n",
    "                  validationset_data=x_val,\n",
    "                  validationset_labels=y_val,\n",
    "                  testset_enrollment_data=x_test_enroll,\n",
    "                  testset_enrollment_labels=y_test_enroll,\n",
    "                  testset_matching_data=x_test_match,\n",
    "                  testset_matching_labels=y_test_match,\n",
    "                  normalization_check=True)\n",
    "\n",
    "paradigm_3 = TSCParadigm(evaluation_methods=[TSCIdentification()], prediction_model=dl4tsc_inception(nb_epochs=10))\n",
    "paradigm_3.evaluate(dataset)\n",
    "\n",
    "paradigm_4 = TSCParadigm(evaluation_methods=[TSCIdentification()],\n",
    "                         prediction_model=TSCKerasModel(model_description=dl4tsc_mlp, nb_epochs=10))\n",
    "paradigm_4.evaluate(dataset)\n",
    "\n",
    "paradigm_5 = AnomalyParadigm(evaluation_methods=[AnomalyIdentification()], prediction_model=AnomalyKerasModel(nb_epochs=5))\n",
    "paradigm_5.evaluate(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the exact configuration of a previously evaluated paradigm to recreate the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_path = os.path.join(paradigm_4.output_dir, f\"paradigm_{paradigm_4.name}.pickle\")\n",
    "paradigm_6 = TSCParadigm.restore(load_path, 'restored_paradigm', '_useckit_out_reruns')\n",
    "paradigm_6.evaluate(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UsecKit as part of a biometric system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below example uses the TSCIdentificationModel from the time_series_classification.evaluation_methods package, but the principle is the same when working with the other Identification models or the IdentificationOrReject and Verification models implemented in the other paradigm's evaluation_methods packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from useckit.paradigms.time_series_classification.evaluation_methods.identification import TSCIdentificationModel\n",
    "\n",
    "identification_model = TSCIdentificationModel(dataset, paradigm_6.prediction_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can identify additional samples using the identification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x_train, _ = make_some_intelligent_noise(labels=1, shape=(1, 10, 10))\n",
    "sample = x_train[0]\n",
    "print(identification_model.identify(np.array([sample])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below more complicated example shows how to do verification with the prediction model from paradigm 5. Note the extreme definition of the rejection thresholds, which will completely determine the result of the verification, no matter the actual predictions made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from useckit.paradigms.anomaly_detection.evaluation_methods.verification import AnomalyVerificationModel\n",
    "\n",
    "verification_model = AnomalyVerificationModel(anomaly_prediction_model=paradigm_5.prediction_model,\n",
    "                                              rejection_thresholds_per_trained_user=[10, 10, 10, 0])\n",
    "\n",
    "all_identities = np.unique(dataset.gather_labels())\n",
    "for i in all_identities:\n",
    "    reverse_transformed_identity = dataset.reverse_label_transform(np.array([i]))\n",
    "    verified = verification_model.verify(np.array([sample]), np.array([i]))\n",
    "    print(f'The generated sample belongs to {reverse_transformed_identity}: {verified}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Notebook execution finished.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
